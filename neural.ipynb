{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Furkan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "application/javascript": "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import jovian\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.73388029088197\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df = pd.read_csv(\"file_name.csv\")\n",
    "df['length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df.head()\n",
    "print( np.mean(df['length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 43255\n",
      "num_words after: 17548\n"
     ]
    }
   ],
   "source": [
    "tok = spacy.load('en_core_web_sm')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n",
    "\n",
    "counts = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    counts.update(tokenize(row['text']))\n",
    "    \n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 17308, 0: 9507})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "    \n",
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "\n",
    "df['encoded'] = df['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index ), dtype=object))\n",
    "df.head()\n",
    "Counter(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['encoded'])\n",
    "y = list(df['labels'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)\n",
    "\n",
    "batch_size = 5000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs +1 ):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        train_acc = 0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            _, pred = torch.max(y_pred.data, 1)\n",
    "            train_acc += (pred == y).sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl, i)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, 100 * val_loss, val_acc, val_rmse))\n",
    "        tra_accur = 100 * train_acc / len(train_ds)\n",
    "        if i % 1 == 0:\n",
    "            print(f'Epoch: {i+1}, training accuracy: {tra_accur}')\n",
    "\n",
    "def conf_matrix_table(cf_matrix):\n",
    "    group_names = ['TN','FP','FN','TP']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, cf_matrix.flatten(), group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "def validation_metrics (model, valid_dl, i):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true= []\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        y_pred.extend(pred.data.numpy())\n",
    "        y_true.extend(y.data.numpy())\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    conf_matrix_table(cf_matrix=cf_matrix)\n",
    "    if i == 30:\n",
    "        print(\"conf matrix: \\n\",cf_matrix )\n",
    "\n",
    "    return sum_loss/total, correct/total, sum_rmse/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training accuracy: 49.664366958791724\n",
      "train loss 0.765, val loss 69.550, val accuracy 0.399, and val rmse 0.775\n",
      "Epoch: 2, training accuracy: 57.08092485549133\n",
      "Epoch: 3, training accuracy: 58.99683013238859\n",
      "Epoch: 4, training accuracy: 64.60469886257691\n",
      "Epoch: 5, training accuracy: 64.60003729256013\n",
      "Epoch: 6, training accuracy: 64.64199142271117\n",
      "train loss 0.646, val loss 64.547, val accuracy 0.642, and val rmse 0.598\n",
      "Epoch: 7, training accuracy: 64.65131456274473\n",
      "Epoch: 8, training accuracy: 64.64665299272795\n",
      "Epoch: 9, training accuracy: 64.64665299272795\n",
      "Epoch: 10, training accuracy: 64.66529927279508\n",
      "Epoch: 11, training accuracy: 64.66529927279508\n",
      "train loss 0.617, val loss 58.978, val accuracy 0.689, and val rmse 0.558\n",
      "Epoch: 12, training accuracy: 64.6606377027783\n",
      "Epoch: 13, training accuracy: 69.14972962893903\n",
      "Epoch: 14, training accuracy: 72.31027410031699\n",
      "Epoch: 15, training accuracy: 76.24930076449748\n",
      "Epoch: 16, training accuracy: 79.19541301510348\n",
      "train loss 0.358, val loss 36.075, val accuracy 0.853, and val rmse 0.384\n",
      "Epoch: 17, training accuracy: 85.86145813910125\n",
      "Epoch: 18, training accuracy: 88.8961402200261\n",
      "Epoch: 19, training accuracy: 90.61625955621854\n",
      "Epoch: 20, training accuracy: 88.51389147865001\n",
      "Epoch: 21, training accuracy: 89.99627074398657\n",
      "train loss 0.211, val loss 25.147, val accuracy 0.904, and val rmse 0.309\n",
      "Epoch: 22, training accuracy: 92.22916278202499\n",
      "Epoch: 23, training accuracy: 93.13816893529741\n",
      "Epoch: 24, training accuracy: 93.9259742681335\n",
      "Epoch: 25, training accuracy: 94.60190192056685\n",
      "Epoch: 26, training accuracy: 95.16129032258064\n",
      "train loss 0.130, val loss 21.243, val accuracy 0.920, and val rmse 0.283\n",
      "Epoch: 27, training accuracy: 95.45963080365468\n",
      "Epoch: 28, training accuracy: 96.05631176580272\n",
      "Epoch: 29, training accuracy: 96.36397538691031\n",
      "Epoch: 30, training accuracy: 96.63434644788364\n",
      "conf matrix: \n",
      " [[1715  205]\n",
      " [ 223 3220]]\n",
      "Epoch: 31, training accuracy: 96.9233637889241\n"
     ]
    }
   ],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 25)\n",
    "        self.linear2 = nn.Linear(25, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear2(self.linear(ht[-1]))\n",
    "    \n",
    "model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)\n",
    "\n",
    "train_model(model_fixed, epochs=30, lr=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8a8c919bd6732bc823198b1f46ac0e8ccd3587ce249f818d866e9fc28155d7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
